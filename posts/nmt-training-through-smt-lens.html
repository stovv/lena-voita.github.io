<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/44a48044965cc004.css" as="style"/><link rel="stylesheet" href="/_next/static/css/44a48044965cc004.css" data-n-g=""/><link rel="preload" href="/_next/static/css/8764ea50cea876ea.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8764ea50cea876ea.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-fc7d2f0e2098927e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-98c6afcf3a360602.js" defer=""></script><script src="/_next/static/chunks/252f366e-32c0ef02bbec04ec.js" defer=""></script><script src="/_next/static/chunks/8b531612-536b98366e47f925.js" defer=""></script><script src="/_next/static/chunks/d64684d8-6f41d6cb185210ac.js" defer=""></script><script src="/_next/static/chunks/626-8f00c7eea2203cfc.js" defer=""></script><script src="/_next/static/chunks/547-9b2136a49167c5ee.js" defer=""></script><script src="/_next/static/chunks/119-1e8e9ac81652ac35.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-64b1235861c92fc3.js" defer=""></script><script src="/_next/static/aitngYHlm5ehRrVt2fHoE/_buildManifest.js" defer=""></script><script src="/_next/static/aitngYHlm5ehRrVt2fHoE/_ssgManifest.js" defer=""></script><script src="/_next/static/aitngYHlm5ehRrVt2fHoE/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__CcNN1"><div class="Header_wrapper__YAiPg"><a class="Header_logo__M2Ygq" href="/">Lena Voita</a><nav class="Header_navigation__8nfJC"><ul><li><a class="" href="/posts">Blog</a></li><li><a class="" href="/papers">Publications</a></li><li><a class="" href="/talks">Talks &amp; Service</a></li><li><a class="" href="/nlp-course">NLP Course | For You</a></li></ul><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 20 20" class="Header_closeIcon___NJuS Header_menuIcon__Dr6iF" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></nav><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 20 20" class="Header_menuIcon__Dr6iF" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></div></header><div class="BaseTemplate_wrapper__JOBXv"><main class="BaseTemplate_main___I0ma"><h1>NMT Training through the Lens of SMT</h1><article><h3 class="Preview_heading__m5V5x"></h3><span style="font-size:0.9em" class="Clarify_md__AlJNy"><p>This is a post for the EMNLP 2021 paper
<a href="https://arxiv.org/abs/2109.01396">Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT.</a></p></span><div class="Side_root__bSrHJ"><div class="Side_main__0xC89 Side_md__YoZwN"><p>In SMT, model competences are modelled with distinct models.
In NMT, the whole translation task is modelled
with a single neural network.
How and when does NMT get to learn all the competences? We show that</p><ul>
<li>during training, NMT undergoes three different stages:</li>
<li>target-side language modeling,</li>
<li>learning how to use source and approaching word-by-word translation,</li>
<li>refining translations, visible by increasingly complex reorderings,
but almost invisible to standard metrics (e.g. BLEU);</li>
</ul></div><div class="Side_side__j9ioX"><p><img src="/posts/nmt-training-through-smt-lens/morda-min.webp" alt="morda-min" class="Image_image__k5v2O"/></p></div></div><ul>
<li>not only this is fun, but it can also help in practice! For example, in settings where
data complexity matters, such as non-autoregressive NMT.</li>
</ul><div class="Preview_bottom__8CJtQ"><div class="Preview_buttons__ha5nu"></div></div></article></main></div><footer class="Footer_root__OEatj"><div class="Footer_wrapper___n7Sm"><div class="Footer_icons__uI7wp"><a href="https://msclogic.illc.uva.nl/people/students/">pic</a><a href="mailto:lena-voita@hotmail.com">pic</a><a href="https://scholar.google.com/citations?user=EcN9o7kAAAAJ">pic</a><a href="https://github.com/lena-voita">pic</a><a href="https://twitter.com/lena_voita">pic</a></div><p>Last updated by </p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"meta":{"layout":"post","title":"NMT Training through the Lens of SMT","image":"/posts/nmt-training-through-smt-lens/morda-min.webp","menu":true,"order":0,"date":"2021-09-01T00:00:00.000Z","paper":"https://arxiv.org/abs/2109.01396","contentTable":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    return _jsx(_Fragment, {});\n  }\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"slug":["posts","nmt-training-through-smt-lens"],"mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      a: \"a\",\n      ul: \"ul\",\n      li: \"li\",\n      img: \"img\"\n    }, _provideComponents(), props.components), {Preview, Cl, SideContainer, Main, Side} = _components;\n    if (!Cl) _missingMdxReference(\"Cl\", true);\n    if (!Main) _missingMdxReference(\"Main\", true);\n    if (!Preview) _missingMdxReference(\"Preview\", true);\n    if (!Side) _missingMdxReference(\"Side\", true);\n    if (!SideContainer) _missingMdxReference(\"SideContainer\", true);\n    return _jsxs(Preview, {\n      children: [_jsx(Cl, {\n        size: \"md\",\n        children: _jsxs(_components.p, {\n          children: [\"This is a post for the EMNLP 2021 paper\\n\", _jsx(_components.a, {\n            href: \"https://arxiv.org/abs/2109.01396\",\n            children: \"Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT.\"\n          })]\n        })\n      }), _jsxs(SideContainer, {\n        children: [_jsxs(Main, {\n          md: true,\n          children: [_jsx(_components.p, {\n            children: \"In SMT, model competences are modelled with distinct models.\\nIn NMT, the whole translation task is modelled\\nwith a single neural network.\\nHow and when does NMT get to learn all the competences? We show that\"\n          }), _jsxs(_components.ul, {\n            children: [\"\\n\", _jsx(_components.li, {\n              children: \"during training, NMT undergoes three different stages:\"\n            }), \"\\n\", _jsx(_components.li, {\n              children: \"target-side language modeling,\"\n            }), \"\\n\", _jsx(_components.li, {\n              children: \"learning how to use source and approaching word-by-word translation,\"\n            }), \"\\n\", _jsx(_components.li, {\n              children: \"refining translations, visible by increasingly complex reorderings,\\nbut almost invisible to standard metrics (e.g. BLEU);\"\n            }), \"\\n\"]\n          })]\n        }), _jsx(Side, {\n          children: _jsx(_components.p, {\n            children: _jsx(_components.img, {\n              src: \"/posts/nmt-training-through-smt-lens/morda-min.webp\",\n              alt: \"morda-min\"\n            })\n          })\n        })]\n      }), _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"not only this is fun, but it can also help in practice! For example, in settings where\\ndata complexity matters, such as non-autoregressive NMT.\"\n        }), \"\\n\"]\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["posts","nmt-training-through-smt-lens"]},"buildId":"aitngYHlm5ehRrVt2fHoE","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>