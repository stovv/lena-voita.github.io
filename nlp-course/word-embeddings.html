<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/80a30c472f65caa8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/80a30c472f65caa8.css" data-n-g=""/><link rel="preload" href="/_next/static/css/0404f90084a82227.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0404f90084a82227.css" data-n-p=""/><link rel="preload" href="/_next/static/css/30b9f6d62a363800.css" as="style"/><link rel="stylesheet" href="/_next/static/css/30b9f6d62a363800.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-d9d824c6edf95fe2.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-fc7d2f0e2098927e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2716c62db23a50d3.js" defer=""></script><script src="/_next/static/chunks/252f366e-32c0ef02bbec04ec.js" defer=""></script><script src="/_next/static/chunks/8b531612-536b98366e47f925.js" defer=""></script><script src="/_next/static/chunks/b5f2ed29-06c05a394cfa0fd0.js" defer=""></script><script src="/_next/static/chunks/d64684d8-6f41d6cb185210ac.js" defer=""></script><script src="/_next/static/chunks/754-2220133da28122e9.js" defer=""></script><script src="/_next/static/chunks/350-b31c90a459d0689a.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-811a8f8e8939f27d.js" defer=""></script><script src="/_next/static/EI9pVGJ5rW9mnqYgy4RyH/_buildManifest.js" defer=""></script><script src="/_next/static/EI9pVGJ5rW9mnqYgy4RyH/_ssgManifest.js" defer=""></script><script src="/_next/static/EI9pVGJ5rW9mnqYgy4RyH/_middlewareManifest.js" defer=""></script><style id="__jsx-1450287905">@media screen and (max-width:380px){.image.jsx-1450287905{
                    }}</style></head><body><style id="__jsx-1450287905">@media screen and (max-width:380px){.image.jsx-1450287905{
                    }}</style><div id="__next"><header class="Header_header__CcNN1"><div class="Header_wrapper__YAiPg"><a class="Header_logo__M2Ygq" href="/">Lena Voita</a><nav class="Header_navigation__8nfJC"><ul><li><a class="" href="/posts">Blog</a></li><li><a class="" href="/papers">Publications</a></li><li><a class="" href="/talks">Talks &amp; Service</a></li><li><a class="" href="/nlp-course">NLP Course | For You</a></li></ul><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 20 20" class="Header_closeIcon___NJuS Header_menuIcon__Dr6iF" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></nav><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 20 20" class="Header_menuIcon__Dr6iF" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></div></header><div class="Course_root___uDJd"><div class="Course_left__DAm6B"><div class="ContentTable_mini__7RtTx"><button class="ContentTable_openButton__tBlaV"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="32" width="32" xmlns="http://www.w3.org/2000/svg"><line x1="17" y1="10" x2="3" y2="10"></line><line x1="21" y1="6" x2="3" y2="6"></line><line x1="21" y1="14" x2="3" y2="14"></line><line x1="17" y1="18" x2="3" y2="18"></line></svg></button></div><div class="ContentTable_root__m8MUB"><button class="ContentTable_closeButton__BkAbc"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="32" width="32" xmlns="http://www.w3.org/2000/svg"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg></button><ul>
<li class=""><a href="#nlp-course-for-you"><strong>NLP Course</strong> <span style="color:var(--green)"><strong>| For You</strong></span> <img src="/nlp-course/ico/logo.webp" style="float:right;width:24px" class="jsx-1450287905 image Image_image__k5v2O"/></a></li>
<li class=""><strong>Word Embeddings</strong></li>
<li class=""><a href="#represent-as-discrete-symbols-one-hot-vectors">One-Hot Vectors</a></li>
<li class=""><a href="#distributional-semantics">Distributional Semantics</a></li>
<li class=""><a href="#">Count-Based Methods</a></li>
<li class="ContentTable_chapter__8klK_ ContentTable_active__L08qs"><a href="#">Word2Vec</a>
<ul>
<li class=""><a href="#">Idea</a></li>
<li class=""><a href="#">Objective Function</a></li>
<li class=""><a href="#">Training Procedure</a></li>
<li class=""><a href="#">Negative Sampling</a></li>
<li class=""><a href="#">Skip-Gram vs CBOW</a></li>
<li class=""><a href="#">Additional Notes</a></li>
</ul>
<svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="ContentTable_chapterIcon__mAjIZ" height="24" width="24" xmlns="http://www.w3.org/2000/svg"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M8.59 16.59L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.41z"></path></svg></li>
<li class=""><a href="#">GloVe</a></li>
<li class=""><a href="#">Evaluation</a></li>
<li class=""><a href="#">Analysis and Interpretability</a></li>
<li class=""><a href="#">Research Thinking</a></li>
<li class=""><a href="#">Related Papers</a></li>
<li class=""><a href="#">Have Fun!</a></li>
</ul></div></div><div class="Course_wrapper__r1p0g"><main><h1>Word Embeddings</h1>
<img height="200" src="/nlp-course/word_emb/word_repr_intro-min.png" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>The way machine learning models &quot;<strong>see</strong>&quot; data is different from how we (humans) do. For example, we can easily
understand the text <strong>&quot;I saw a cat&quot;</strong>,
but our models can not - they need vectors of features.
Such vectors, or <strong>word embeddings</strong>, are representations of words which can be fed into your model.</p>
<br/>
<img height="130" src="/nlp-course/word_emb/lookup_table.gif" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p><u>How it works:</u> Look-up Table (Vocabulary)</p>
<p>In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance.
For each vocabulary word, a look-up table contains its embedding. This embedding can be found
using the word index in the vocabulary (i.e., you
to <strong>look up</strong> the embedding in the table using word index).</p>
<img height="70" src="/nlp-course/word_emb/unk_in_voc-min.png" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary
contains a special token <strong>UNK</strong>. Alternatively, unknown tokens can be ignored or assigned a zero vector.</p>
<h5 id="the-main-question-of-this-lecture-is-how-do-we-get-these-word-vectors">The main question of this lecture is: how do we get these word vectors?</h5>
<br/>
<br/>
<h4 id="represent-as-discrete-symbols-one-hot-vectors">Represent as Discrete Symbols: One-hot Vectors</h4>
<p>The easiest you can do is to represent words as one-hot vectors: for the i-th word in the vocabulary,
the vector has 1 on the i-th dimension and 0 on the rest. In Machine Learning, this is the most simple way to represent
categorical features.</p>
<p>You probably can guess why one-hot vectors are not the best way to represent words. One of the problems is that
for large vocabularies, these vectors will be very long: vector dimensionality is equal to the vocabulary size.
This is undesirable in practice, but this problem is not the most crucial one.</p>
<p>What is really important, is that these vectors <strong>know nothing</strong>
about the words they represent. For example, one-hot vectors &quot;think&quot; that <strong>cat</strong> is as close to <strong>dog</strong> as it is to <strong>table!</strong>
We can say that <u>one-hot vectors do not capture <strong>meaning.</strong></u></p>
<p>But how do we know what is meaning?</p>
<br/>
<br/>
<h4 id="distributional-semantics">Distributional Semantics</h4>
<p>To capture meaning of words in their vectors, we first need to define
the notion of meaning that can be used in practice.
For this, let us try to understand how we, humans, get to know which words have similar meaning.</p>
<div class="Part_root__tnD2L"><div><div class="VDivider_root__wwrID"><img src="/nlp-course/ico/paw_empty.webp" class="jsx-1450287905 image Image_image__k5v2O VDivider_image__c_hrl"/><div style="background-color:var(--green)" class="VDivider_line__eTm9Y"></div><img src="/nlp-course/ico/paw_empty.webp" class="jsx-1450287905 image Image_image__k5v2O VDivider_image__c_hrl"/></div></div><div class="Part_content__9ds4r"><div class="Callout_root__UWz2T"><div class="Callout_container__wJQ9a" style="border-color:var(--green)"><p><u>How to:</u> go over the slides at your pace. Try to notice how your brain works.</p></div></div><div class="CourseCarousel_root__HR4vs"><div class="Carousel_root__hDBl7 CourseCarousel_carousel__REbwo"><div class="slick-slider slick-initialized" dir="ltr"><button type="button" data-role="none" class="slick-arrow slick-prev" style="display:block"> <!-- -->Previous</button><div class="slick-list"><div class="slick-track" style="width:1900%;left:-100%"><div data-index="-1" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="0" class="slick-slide slick-active slick-current" tabindex="-1" aria-hidden="false" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino1-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="1" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course//word_emb/tezguino2-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="2" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino3-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="3" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino4-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="4" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino5-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="5" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino6-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="6" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino7-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="7" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino8-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="8" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="9" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino1-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="10" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course//word_emb/tezguino2-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="11" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino3-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="12" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino4-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="13" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino5-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="14" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino6-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="15" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino7-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="16" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino8-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="17" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div></div></div><button type="button" data-role="none" class="slick-arrow slick-next" style="display:block"> <!-- -->Next</button><ul style="display:block" class="slick-dots"><li class="slick-active"><button>1</button></li><li class=""><button>2</button></li><li class=""><button>3</button></li><li class=""><button>4</button></li><li class=""><button>5</button></li><li class=""><button>6</button></li><li class=""><button>7</button></li><li class=""><button>8</button></li><li class=""><button>9</button></li></ul></div></div></div><span style="font-size:0.9em" class="Clarify_sx__WKZPq Clarify_gray__OPWqV"><p><u>Lena</u>: The example is from <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Jacob Eisenstein&#x27;s NLP notes;</a>
the <strong>tezgüino</strong> example originally appeared in <a href="https://www.aclweb.org/anthology/C98-2122.pdf">Lin, 1998</a>.</p></span></div></div>
<p>Once you saw how the unknown word used in different contexts, you were able to understand it&#x27;s meaning.
How did you do this?</p>
<p>The hypothesis is that your brain searched for other words
that can be used in the same contexts, found some (e.g., <strong>wine</strong>), and
made a conclusion that <strong>tezgüino</strong> has meaning similar to those other words.
This is the <span style="font-size:0.9em" class="Clarify_sx__WKZPq Clarify_comic__DLOtf">distributional hypothesis:</span></p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p>Words which frequently appear in <strong>similar contexts</strong> have <strong>similar meaning</strong>.</p>
</q></div></div>
<p>Once you saw how the unknown word used in different contexts,
you were able to understand it&#x27;s meaning.
How did you do this?</p>
<p>The hypothesis is that your brain searched for other words
that can be used in the same contexts, found some (e.g., <strong>wine</strong>), and
made a conclusion that <strong>tezgüino</strong> has meaning
similar to those other words.</p>
<p>This is the <span style="font-size:0.9em" class="Clarify_sx__WKZPq Clarify_comic__DLOtf">distributional hypothesis:</span></p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p>Words which frequently appear in <strong>similar contexts</strong> have <strong>similar meaning</strong>.</p>
</q></div></div>
<span style="font-size:0.9em" class="Clarify_sx__WKZPq Clarify_comic__DLOtf Clarify_gray__OPWqV"><p><u>Lena:</u> Often you can find it formulated as &quot;You shall know a word by the company it keeps&quot; with the reference
to J. R. Firth in 1957, but
actually there
were a lot more people
responsible, and much earlier. For example, <a href="https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520">Harris, 1954.</a></p></span>
<p>This is an extremely valuable idea: it can be used in practice to make word vectors capture
their meaning. According to the distributional hypothesis, &quot;to capture meaning&quot; and
&quot;to capture contexts&quot; are inherently the same.
Therefore, all we need to do is to put information about word contexts into word representation.</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>Main idea</u>: We need to put information about word contexts into word representation.</p>
</q></div></div>
<p>All we&#x27;ll be doing at this lecture is looking at different ways to do this.</p>
<h2 id="count-based-methods">Count-Based Methods</h2>
<p><img src="/nlp-course/word_emb/preneural/idea-min.png" alt="count-based" class="jsx-1450287905 image Image_image__k5v2O"/></p>
<p>Let&#x27;s remember our main idea:</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>Main idea</u>: We have to put information about contexts into word vectors.</p>
</q></div></div>
<p>Count-based methods take this idea quite literally:</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>How</u>: Put this information <strong>manually</strong> based on global corpus statistics.</p>
</q></div></div>
<p>The general procedure is illustrated above and consists of the two steps: (1)
construct a word-context matrix, (2) reduce its dimensionality. There are two reasons to reduce dimensionality.
First, a raw matrix is very large. Second, since a lot of words appear in only a few of possible contexts,
this matrix potentially has a lot of uninformative elements (e.g., zeros).</p>
<p>To estimate
similarity between words/contexts, usually you need to evaluate
the dot-product of normalized word/context vectors (i.e., cosine similarity).</p>
<img src="/nlp-course/word_emb/preneural/need_to_define-min.png" style="width:25%" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>To define a count-based method, we need to define two things:</p>
<ul>
<li>possible contexts (including what does it mean that a word appears in a context),</li>
<li>the notion of association, i.e., formulas for computing matrix elements.</li>
</ul>
<p>Below we provide a couple of popular ways of doing this.</p>
<h3 id="simple-co-occurence-counts">Simple: Co-Occurence Counts</h3>
<br/>
<img src="/nlp-course/word_emb/preneural/window-min.png" class="jsx-1450287905 image Image_image__k5v2O"/>
<img src="/nlp-course/word_emb/preneural/define_simple-min.png" style="width:25%" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>The simplest approach is to define contexts as each word in an L-sized window.
Matrix element for a word-context pair (w, c) is the number of times w appears in context c.
This is the very basic (and very, very old) method for obtaining embeddings.</p>
<div class="Callout_root__UWz2T"><img src="/nlp-course/ico/bulb_empty.webp" class="jsx-1450287905 image Image_image__k5v2O Callout_icon__oicYr"/><div class="Callout_container__wJQ9a" style="border-color:var(--yellow)"><p>The (once) famous HAL model (1996)
is also a modification of this approach.
Learn more from <a href="#research_improve_count_based">this exercise</a>
in the <a href="#research_thinking">Research Thinking</a> section.</p></div></div>
<h3 id="positive-pointwise-mutual-information-ppmi">Positive Pointwise Mutual Information (PPMI)</h3>
<img src="/nlp-course/word_emb/preneural/define_ppmi-min.png" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>Here contexts are defined as before, but the measure of
the association between word and context is more clever: positive PMI (or PPMI for short).
PPMI measure is widely regarded as state-of-the-art for pre-neural distributional-similarity models.</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>Important</u>: relation to neural models! Turns out, some of the neural methods we will consider (Word2Vec) were shown
to implicitly approximate the factorization of a (shifted) PMI matrix. Stay tuned!</p>
</q></div></div>
<br/>
<h3 id="latent-semantic-analysis-lsa-understanding-documents">Latent Semantic Analysis (LSA): Understanding Documents</h3>
<img src="/nlp-course/word_emb/preneural/lsa-min.png" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p><a href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf">Latent Semantic Analysis (LSA)</a> analyzes a collection of documents.
While in the previous approaches contexts served only to get word vectors and were thrown away afterward, here we are
also interested in context, or, in this case, document vectors. LSA is one of the simplest topic models:
cosine similarity between document vectors can be used to measure similarity between documents.
The term &quot;LSA&quot; sometimes refers to a more general approach of applying SVD to a term-document
matrix where the term-document elements can be computed in different ways
(e.g., simple co-occurrence, tf-idf, or some other weighting).</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>Animation alert!</u>
<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">LSA wikipedia page</a> has a nice
animation of the topic detection process in a document-word matrix - take a look!</p>
</q></div></div>
<h2 id="word-2-vec-a-prediction-based-method">Word2Vec: a Prediction-Based Method</h2>
<div id="w2v_idea"></div>
<p>Let us remember our main idea again:</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>Main idea</u>: We have to put information about contexts into word vectors.</p>
</q></div></div>
<p>While count-based methods took this idea quite literally, Word2Vec uses it in a different manner:</p>
<div class="Quote_root__yMaLR"><div class="Quote_wrapper__kADgh"><q>
<p><u>How</u>: <strong>Learn</strong> word vectors by teaching them to <strong>predict contexts</strong>.</p>
</q></div></div>
<img src="/nlp-course/word_emb/w2v/intro-min.png" class="jsx-1450287905 image Image_image__k5v2O Image_right__dckIr"/>
<p>Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively
for a certain objective. The objective forces word vectors to &quot;know&quot; contexts a word can appear in:
the vectors are trained to predict possible contexts of the corresponding words.
As you remember from the distributional hypothesis, if vectors &quot;know&quot; about contexts, they &quot;know&quot; word meaning.</p>
<p>Word2Vec is an iterative method. Its main idea is as follows:</p>
<ul>
<li>take a huge text corpus;</li>
<li>go over the text with a sliding window, moving one word at a time. At each step, there is a central word
and context words (other words in this window);</li>
<li>for the central word, compute probabilities of context words;</li>
<li>adjust the vectors to increase these probabilities.</li>
</ul>
<div class="Part_root__tnD2L"><div><div class="VDivider_root__wwrID"><img src="/nlp-course/ico/paw_empty.webp" class="jsx-1450287905 image Image_image__k5v2O VDivider_image__c_hrl"/><div style="background-color:var(--green)" class="VDivider_line__eTm9Y"></div><img src="/nlp-course/ico/paw_empty.webp" class="jsx-1450287905 image Image_image__k5v2O VDivider_image__c_hrl"/></div></div><div class="Part_content__9ds4r"><p><u>How to:</u> go over the illustration to understand the main idea.</p><div class="CourseCarousel_root__HR4vs"><div class="Carousel_root__hDBl7 CourseCarousel_carousel__REbwo"><div class="slick-slider slick-initialized" dir="ltr"><button type="button" data-role="none" class="slick-arrow slick-prev" style="display:block"> <!-- -->Previous</button><div class="slick-list"><div class="slick-track" style="width:1900%;left:-100%"><div data-index="-1" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="0" class="slick-slide slick-active slick-current" tabindex="-1" aria-hidden="false" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob1-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="1" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob2-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="2" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob3-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="3" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob4-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="4" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob5-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="5" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob6-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="6" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino7-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="7" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino8-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="8" class="slick-slide" tabindex="-1" aria-hidden="true" style="outline:none;width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="9" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob1-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="10" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob2-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="11" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob3-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="12" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob4-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="13" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob5-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="14" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/w2v/window_prob6-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="15" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino7-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="16" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino8-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div><div data-index="17" tabindex="-1" class="slick-slide slick-cloned" aria-hidden="true" style="width:5.2631578947368425%"><div><img src="/nlp-course/word_emb/tezguino9-min.png" class="Carousel_image__of8ah" tabindex="-1" style="width:100%;display:inline-block"/></div></div></div></div><button type="button" data-role="none" class="slick-arrow slick-next" style="display:block"> <!-- -->Next</button><ul style="display:block" class="slick-dots"><li class="slick-active"><button>1</button></li><li class=""><button>2</button></li><li class=""><button>3</button></li><li class=""><button>4</button></li><li class=""><button>5</button></li><li class=""><button>6</button></li><li class=""><button>7</button></li><li class=""><button>8</button></li><li class=""><button>9</button></li></ul></div></div></div><span style="font-size:0.9em" class="Clarify_sx__WKZPq Clarify_gray__OPWqV"><p><u>Lena:</u> Visualization idea is from <a href="http://web.stanford.edu/class/cs224n/index.html#schedule">the Stanford CS224n course.</a></p></span></div></div>
<div id="w2v_objective_function"></div>
<h3 id="objective-function-negative-log-likelihood"><u>Objective Function</u>: Negative Log-Likelihood</h3>
<p>For each position <span data-testid="react-katex"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t =1, \dots, T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span></span> in a text corpus,
Word2Vec predicts context words within a m-sized window given the central
word <span data-testid="react-katex"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#88bd33"><msub><mi>w</mi><mi>t</mi></msub></mstyle></mrow><annotation encoding="application/x-tex">\color{#88bd33}{w_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord" style="color:#88bd33;"><span class="mord" style="color:#88bd33;"><span class="mord mathnormal" style="margin-right:0.02691em;color:#88bd33;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:#88bd33;"><span class="mord mathnormal mtight" style="color:#88bd33;">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span>:</p>
<br/>
<div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mstyle mathcolor="#88bd33"><mrow><mi>L</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>h</mi><mi>o</mi><mi>o</mi><mi>d</mi></mrow><mstyle mathcolor="black"><mo>=</mo><mi>L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><munder><mo>∏</mo><mrow><mo>−</mo><mi>m</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>m</mi><mo separator="true">,</mo><mi>j</mi><mo mathvariant="normal">≠</mo><mn>0</mn></mrow></munder><mi>P</mi><mo stretchy="false">(</mo><mstyle mathcolor="#888"><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><mstyle mathcolor="#88bd33"><msub><mi>w</mi><mi>t</mi></msub><mstyle mathcolor="black"><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mstyle></mstyle></mstyle></mstyle></mstyle></mrow><annotation encoding="application/x-tex">\color{#88bd33}{{Likelihood}} \color{black}= L(\theta)=\prod\limits_{t=1}^T\prod\limits_{-m\le j \le m, j\neq 0}P(\color{#888}{w_{t+j}}|\color{#88bd33}{w_t}\color{black}, \theta),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord" style="color:#88bd33;"><span class="mord" style="color:#88bd33;"><span class="mord mathnormal" style="color:#88bd33;">L</span><span class="mord mathnormal" style="margin-right:0.03148em;color:#88bd33;">ik</span><span class="mord mathnormal" style="color:#88bd33;">e</span><span class="mord mathnormal" style="margin-right:0.01968em;color:#88bd33;">l</span><span class="mord mathnormal" style="color:#88bd33;">ih</span><span class="mord mathnormal" style="color:#88bd33;">oo</span><span class="mord mathnormal" style="color:#88bd33;">d</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel" style="color:black;">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="color:black;">L</span><span class="mopen" style="color:black;">(</span><span class="mord mathnormal" style="margin-right:0.02778em;color:black;">θ</span><span class="mclose" style="color:black;">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel" style="color:black;">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.2666em;vertical-align:-1.4382em;"></span><span class="mop op-limits" style="color:black;"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight" style="color:black;"><span class="mord mtight" style="color:black;"><span class="mord mathnormal mtight" style="color:black;">t</span><span class="mrel mtight" style="color:black;">=</span><span class="mord mtight" style="color:black;">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op" style="color:black;">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight" style="color:black;"><span class="mord mathnormal mtight" style="margin-right:0.13889em;color:black;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits" style="color:black;"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight" style="color:black;"><span class="mord mtight" style="color:black;"><span class="mord mtight" style="color:black;">−</span><span class="mord mathnormal mtight" style="color:black;">m</span><span class="mrel mtight" style="color:black;">≤</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;color:black;">j</span><span class="mrel mtight" style="color:black;">≤</span><span class="mord mathnormal mtight" style="color:black;">m</span><span class="mpunct mtight" style="color:black;">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;color:black;">j</span><span class="mrel mtight" style="color:black;"><span class="mrel mtight" style="color:black;"><span class="mord vbox mtight" style="color:black;"><span class="thinbox mtight" style="color:black;"><span class="rlap mtight" style="color:black;"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord mtight" style="color:black;"><span class="mrel mtight" style="color:black;"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight" style="color:black;">=</span></span><span class="mord mtight" style="color:black;">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op" style="color:black;">∏</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4382em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;color:black;">P</span><span class="mopen" style="color:black;">(</span><span class="mord" style="color:#888;"><span class="mord" style="color:#888;"><span class="mord mathnormal" style="margin-right:0.02691em;color:#888;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:#888;"><span class="mord mtight" style="color:#888;"><span class="mord mathnormal mtight" style="color:#888;">t</span><span class="mbin mtight" style="color:#888;">+</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;color:#888;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span><span class="mord" style="color:#888;">∣</span><span class="mord" style="color:#88bd33;"><span class="mord" style="color:#88bd33;"><span class="mord mathnormal" style="margin-right:0.02691em;color:#88bd33;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:#88bd33;"><span class="mord mathnormal mtight" style="color:#88bd33;">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mpunct" style="color:black;">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;color:black;">θ</span><span class="mclose" style="color:black;">)</span><span class="mpunct" style="color:black;">,</span></span></span></span></span></div>
<p>where <span data-testid="react-katex"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span> are all variables to be optimized.</p>
<img src="/nlp-course/word_emb/w2v/loss_with_the_plan-min.png" class="jsx-1450287905 image Image_image__k5v2O"/>
<p>Note how well the loss agrees with our plan main above: go over text with a
sliding window and compute probabilities.
Now let&#x27;s find out how to compute these probabilities.</p>
<div id="word2vec_calculate_p"></div>
<h4 id="how-to-calculate-p-color-888-w-t-j-color-black-color-88-bd-33-w-t-color-black-theta"><u>How to calculate</u>  <span data-testid="react-katex"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mstyle mathcolor="#888"><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mstyle mathcolor="black"><mi mathvariant="normal">∣</mi><mstyle mathcolor="#88bd33"><msub><mi>w</mi><mi>t</mi></msub><mstyle mathcolor="black"><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">?</mo></mstyle></mstyle></mstyle></mstyle></mrow><annotation encoding="application/x-tex">P(\color{#888}{w_{t+j}}\color{black}|\color{#88bd33}{w_t}\color{black}, \theta)?</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord" style="color:#888;"><span class="mord" style="color:#888;"><span class="mord mathnormal" style="margin-right:0.02691em;color:#888;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:#888;"><span class="mord mtight" style="color:#888;"><span class="mord mathnormal mtight" style="color:#888;">t</span><span class="mbin mtight" style="color:#888;">+</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;color:#888;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span><span class="mord" style="color:black;">∣</span><span class="mord" style="color:#88bd33;"><span class="mord" style="color:#88bd33;"><span class="mord mathnormal" style="margin-right:0.02691em;color:#88bd33;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:#88bd33;"><span class="mord mathnormal mtight" style="color:#88bd33;">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mpunct" style="color:black;">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;color:black;">θ</span><span class="mclose" style="color:black;">)?</span></span></span></span></span></h4>
<img src="/nlp-course/word_emb/w2v/two_vocs_with_theta-min.png" class="jsx-1450287905 image Image_image__k5v2O"/>
</main></div></div><footer class="Footer_root__OEatj"><div class="Footer_wrapper___n7Sm"><div class="Footer_icons__uI7wp"><a href="https://msclogic.illc.uva.nl/people/students/">pic</a><a href="mailto:lena-voita@hotmail.com">pic</a><a href="https://scholar.google.com/citations?user=EcN9o7kAAAAJ">pic</a><a href="https://github.com/lena-voita">pic</a><a href="https://twitter.com/lena_voita">pic</a></div><p>Last updated by September 16,2022</p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"meta":{"title":"Test","layout":"course","contentTable":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      ul: \"ul\",\n      li: \"li\",\n      a: \"a\",\n      strong: \"strong\"\n    }, _provideComponents(), props.components), {Hl, Img} = _components;\n    if (!Hl) _missingMdxReference(\"Hl\", true);\n    if (!Img) _missingMdxReference(\"Img\", true);\n    return _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsxs(_components.a, {\n          href: \"#nlp-course-for-you\",\n          children: [_jsx(_components.strong, {\n            children: \"NLP Course\"\n          }), \" \", _jsx(Hl, {\n            color: \"green\",\n            children: _jsx(_components.strong, {\n              children: \"| For You\"\n            })\n          }), \" \", _jsx(Img, {\n            src: '/nlp-course/ico/logo.webp',\n            style: {\n              float: 'right',\n              width: '24px'\n            }\n          })]\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.strong, {\n          children: \"Word Embeddings\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#represent-as-discrete-symbols-one-hot-vectors\",\n          children: \"One-Hot Vectors\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#distributional-semantics\",\n          children: \"Distributional Semantics\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Count-Based Methods\"\n        })\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"#\",\n          children: \"Word2Vec\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Idea\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Objective Function\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Training Procedure\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Negative Sampling\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Skip-Gram vs CBOW\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Additional Notes\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"GloVe\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Evaluation\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Analysis and Interpretability\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Research Thinking\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Related Papers\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Have Fun!\"\n        })\n      }), \"\\n\"]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"slug":["nlp-course","word-embeddings"],"mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      strong: \"strong\",\n      h4: \"h4\",\n      h3: \"h3\",\n      a: \"a\",\n      blockquote: \"blockquote\",\n      h1: \"h1\",\n      img: \"img\",\n      ul: \"ul\",\n      li: \"li\",\n      h2: \"h2\"\n    }, _provideComponents(), props.components), {Img, Part, Callout, CourseCarousel, Cl, Math, BlockMath, Quiz} = _components;\n    if (!BlockMath) _missingMdxReference(\"BlockMath\", true);\n    if (!Callout) _missingMdxReference(\"Callout\", true);\n    if (!Cl) _missingMdxReference(\"Cl\", true);\n    if (!CourseCarousel) _missingMdxReference(\"CourseCarousel\", true);\n    if (!Img) _missingMdxReference(\"Img\", true);\n    if (!Math) _missingMdxReference(\"Math\", true);\n    if (!Part) _missingMdxReference(\"Part\", true);\n    if (!Quiz) _missingMdxReference(\"Quiz\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsx(\"h1\", {\n        children: \"Word Embeddings\"\n      }), \"\\n\", _jsx(Img, {\n        height: \"200\",\n        src: \"/nlp-course/word_emb/word_repr_intro-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The way machine learning models \\\"\", _jsx(_components.strong, {\n          children: \"see\"\n        }), \"\\\" data is different from how we (humans) do. For example, we can easily\\nunderstand the text \", _jsx(_components.strong, {\n          children: \"\\\"I saw a cat\\\"\"\n        }), \",\\nbut our models can not - they need vectors of features.\\nSuch vectors, or \", _jsx(_components.strong, {\n          children: \"word embeddings\"\n        }), \", are representations of words which can be fed into your model.\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(Img, {\n        height: \"130\",\n        src: \"/nlp-course/word_emb/lookup_table.gif\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(\"u\", {\n          children: \"How it works:\"\n        }), \" Look-up Table (Vocabulary)\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance.\\nFor each vocabulary word, a look-up table contains its embedding. This embedding can be found\\nusing the word index in the vocabulary (i.e., you\\nto \", _jsx(_components.strong, {\n          children: \"look up\"\n        }), \" the embedding in the table using word index).\"]\n      }), \"\\n\", _jsx(Img, {\n        height: \"70\",\n        src: \"/nlp-course/word_emb/unk_in_voc-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary\\ncontains a special token \", _jsx(_components.strong, {\n          children: \"UNK\"\n        }), \". Alternatively, unknown tokens can be ignored or assigned a zero vector.\"]\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"The main question of this lecture is: how do we get these word vectors?\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h3, {\n        children: \"Represent as Discrete Symbols: One-hot Vectors\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The easiest you can do is to represent words as one-hot vectors: for the i-th word in the vocabulary,\\nthe vector has 1 on the i-th dimension and 0 on the rest. In Machine Learning, this is the most simple way to represent\\ncategorical features.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"You probably can guess why one-hot vectors are not the best way to represent words. One of the problems is that\\nfor large vocabularies, these vectors will be very long: vector dimensionality is equal to the vocabulary size.\\nThis is undesirable in practice, but this problem is not the most crucial one.\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"What is really important, is that these vectors \", _jsx(_components.strong, {\n          children: \"know nothing\"\n        }), \"\\nabout the words they represent. For example, one-hot vectors \\\"think\\\" that \", _jsx(_components.strong, {\n          children: \"cat\"\n        }), \" is as close to \", _jsx(_components.strong, {\n          children: \"dog\"\n        }), \" as it is to \", _jsx(_components.strong, {\n          children: \"table!\"\n        }), \"\\nWe can say that \", _jsxs(\"u\", {\n          children: [\"one-hot vectors do not capture \", _jsx(_components.strong, {\n            children: \"meaning.\"\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"But how do we know what is meaning?\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h3, {\n        children: \"Distributional Semantics\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To capture meaning of words in their vectors, we first need to define\\nthe notion of meaning that can be used in practice.\\nFor this, let us try to understand how we, humans, get to know which words have similar meaning.\"\n      }), \"\\n\", _jsxs(Part, {\n        divider: {\n          topImage: '/nlp-course/ico/paw_empty.webp',\n          botImage: '/nlp-course/ico/paw_empty.webp'\n        },\n        children: [_jsx(Callout, {\n          color: 'var(--green)',\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"How to:\"\n            }), \" go over the slides at your pace. Try to notice how your brain works.\"]\n          })\n        }), _jsx(CourseCarousel, {\n          slides: [{\n            url: '/nlp-course/word_emb/tezguino1-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course//word_emb/tezguino2-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino3-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino4-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino5-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino6-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino7-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino8-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino9-min.png',\n            title: ''\n          }]\n        }), _jsx(Cl, {\n          gray: true,\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"Lena\"\n            }), \": The example is from \", _jsx(_components.a, {\n              href: \"https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf\",\n              children: \"Jacob Eisenstein's NLP notes;\"\n            }), \"\\nthe \", _jsx(_components.strong, {\n              children: \"tezgüino\"\n            }), \" example originally appeared in \", _jsx(_components.a, {\n              href: \"https://www.aclweb.org/anthology/C98-2122.pdf\",\n              children: \"Lin, 1998\"\n            }), \".\"]\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Once you saw how the unknown word used in different contexts, you were able to understand it's meaning.\\nHow did you do this?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The hypothesis is that your brain searched for other words\\nthat can be used in the same contexts, found some (e.g., \", _jsx(_components.strong, {\n          children: \"wine\"\n        }), \"), and\\nmade a conclusion that \", _jsx(_components.strong, {\n          children: \"tezgüino\"\n        }), \" has meaning similar to those other words.\\nThis is the \", _jsx(Cl, {\n          comic: true,\n          children: \"distributional hypothesis:\"\n        })]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Words which frequently appear in \", _jsx(_components.strong, {\n            children: \"similar contexts\"\n          }), \" have \", _jsx(_components.strong, {\n            children: \"similar meaning\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Once you saw how the unknown word used in different contexts,\\nyou were able to understand it's meaning.\\nHow did you do this?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The hypothesis is that your brain searched for other words\\nthat can be used in the same contexts, found some (e.g., \", _jsx(_components.strong, {\n          children: \"wine\"\n        }), \"), and\\nmade a conclusion that \", _jsx(_components.strong, {\n          children: \"tezgüino\"\n        }), \" has meaning\\nsimilar to those other words.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This is the \", _jsx(Cl, {\n          comic: true,\n          children: \"distributional hypothesis:\"\n        })]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Words which frequently appear in \", _jsx(_components.strong, {\n            children: \"similar contexts\"\n          }), \" have \", _jsx(_components.strong, {\n            children: \"similar meaning\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(Cl, {\n        gray: true,\n        comic: true,\n        children: _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Lena:\"\n          }), \" Often you can find it formulated as \\\"You shall know a word by the company it keeps\\\" with the reference\\nto J. R. Firth in 1957, but\\nactually there\\nwere a lot more people\\nresponsible, and much earlier. For example, \", _jsx(_components.a, {\n            href: \"https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\",\n            children: \"Harris, 1954.\"\n          })]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This is an extremely valuable idea: it can be used in practice to make word vectors capture\\ntheir meaning. According to the distributional hypothesis, \\\"to capture meaning\\\" and\\n\\\"to capture contexts\\\" are inherently the same.\\nTherefore, all we need to do is to put information about word contexts into word representation.\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We need to put information about word contexts into word representation.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"All we'll be doing at this lecture is looking at different ways to do this.\"\n      }), \"\\n\", _jsx(_components.h1, {\n        children: \"Count-Based Methods\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"/nlp-course/word_emb/preneural/idea-min.png\",\n          alt: \"count-based\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let's remember our main idea:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We have to put information about contexts into word vectors.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Count-based methods take this idea quite literally:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How\"\n          }), \": Put this information \", _jsx(_components.strong, {\n            children: \"manually\"\n          }), \" based on global corpus statistics.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The general procedure is illustrated above and consists of the two steps: (1)\\nconstruct a word-context matrix, (2) reduce its dimensionality. There are two reasons to reduce dimensionality.\\nFirst, a raw matrix is very large. Second, since a lot of words appear in only a few of possible contexts,\\nthis matrix potentially has a lot of uninformative elements (e.g., zeros).\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To estimate\\nsimilarity between words/contexts, usually you need to evaluate\\nthe dot-product of normalized word/context vectors (i.e., cosine similarity).\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/need_to_define-min.png\",\n        right: true,\n        lightBox: true,\n        style: {\n          width: '25%'\n        }\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To define a count-based method, we need to define two things:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"possible contexts (including what does it mean that a word appears in a context),\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"the notion of association, i.e., formulas for computing matrix elements.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Below we provide a couple of popular ways of doing this.\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Simple: Co-Occurence Counts\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/window-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/define_simple-min.png\",\n        right: true,\n        lightBox: true,\n        style: {\n          width: '25%'\n        }\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The simplest approach is to define contexts as each word in an L-sized window.\\nMatrix element for a word-context pair (w, c) is the number of times w appears in context c.\\nThis is the very basic (and very, very old) method for obtaining embeddings.\"\n      }), \"\\n\", _jsx(Callout, {\n        icon: 'bulb_empty',\n        children: _jsxs(_components.p, {\n          children: [\"The (once) famous HAL model (1996)\\nis also a modification of this approach.\\nLearn more from \", _jsx(_components.a, {\n            href: \"#research_improve_count_based\",\n            children: \"this exercise\"\n          }), \"\\nin the \", _jsx(_components.a, {\n            href: \"#research_thinking\",\n            children: \"Research Thinking\"\n          }), \" section.\"]\n        })\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Positive Pointwise Mutual Information (PPMI)\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/define_ppmi-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Here contexts are defined as before, but the measure of\\nthe association between word and context is more clever: positive PMI (or PPMI for short).\\nPPMI measure is widely regarded as state-of-the-art for pre-neural distributional-similarity models.\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Important\"\n          }), \": relation to neural models! Turns out, some of the neural methods we will consider (Word2Vec) were shown\\nto implicitly approximate the factorization of a (shifted) PMI matrix. Stay tuned!\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h2, {\n        children: \"Latent Semantic Analysis (LSA): Understanding Documents\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/lsa-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(_components.a, {\n          href: \"http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf\",\n          children: \"Latent Semantic Analysis (LSA)\"\n        }), \" analyzes a collection of documents.\\nWhile in the previous approaches contexts served only to get word vectors and were thrown away afterward, here we are\\nalso interested in context, or, in this case, document vectors. LSA is one of the simplest topic models:\\ncosine similarity between document vectors can be used to measure similarity between documents.\\nThe term \\\"LSA\\\" sometimes refers to a more general approach of applying SVD to a term-document\\nmatrix where the term-document elements can be computed in different ways\\n(e.g., simple co-occurrence, tf-idf, or some other weighting).\"]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Animation alert!\"\n          }), \"\\n\", _jsx(_components.a, {\n            href: \"https://en.wikipedia.org/wiki/Latent_semantic_analysis\",\n            children: \"LSA wikipedia page\"\n          }), \" has a nice\\nanimation of the topic detection process in a document-word matrix - take a look!\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.h1, {\n        children: \"Word2Vec: a Prediction-Based Method\"\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"w2v_idea\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let us remember our main idea again:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We have to put information about contexts into word vectors.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"While count-based methods took this idea quite literally, Word2Vec uses it in a different manner:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How\"\n          }), \": \", _jsx(_components.strong, {\n            children: \"Learn\"\n          }), \" word vectors by teaching them to \", _jsx(_components.strong, {\n            children: \"predict contexts\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/intro-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively\\nfor a certain objective. The objective forces word vectors to \\\"know\\\" contexts a word can appear in:\\nthe vectors are trained to predict possible contexts of the corresponding words.\\nAs you remember from the distributional hypothesis, if vectors \\\"know\\\" about contexts, they \\\"know\\\" word meaning.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Word2Vec is an iterative method. Its main idea is as follows:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"take a huge text corpus;\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"go over the text with a sliding window, moving one word at a time. At each step, there is a central word\\nand context words (other words in this window);\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"for the central word, compute probabilities of context words;\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"adjust the vectors to increase these probabilities.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(Part, {\n        divider: {\n          topImage: '/nlp-course/ico/paw_empty.webp',\n          botImage: '/nlp-course/ico/paw_empty.webp'\n        },\n        children: [_jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How to:\"\n          }), \" go over the illustration to understand the main idea.\"]\n        }), _jsx(CourseCarousel, {\n          slides: [{\n            url: '/nlp-course/word_emb/w2v/window_prob1-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob2-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob3-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob4-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob5-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob6-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino7-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino8-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino9-min.png',\n            title: ''\n          }]\n        }), _jsx(Cl, {\n          gray: true,\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"Lena:\"\n            }), \" Visualization idea is from \", _jsx(_components.a, {\n              href: \"http://web.stanford.edu/class/cs224n/index.html#schedule\",\n              children: \"the Stanford CS224n course.\"\n            })]\n          })\n        })]\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"w2v_objective_function\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [_jsx(\"u\", {\n          children: \"Objective Function\"\n        }), \": Negative Log-Likelihood\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"For each position \", _jsx(Math, {\n          children: \"(t =1, \\\\dots, T)\"\n        }), \" in a text corpus,\\nWord2Vec predicts context words within a m-sized window given the central\\nword \", _jsx(Math, {\n          children: '\\\\color{#88bd33}{w_t}'\n        }), \":\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(BlockMath, {\n        children: '\\\\color{#88bd33}{{Likelihood}} \\\\color{black}= L(\\\\theta)=\\\\prod\\\\limits_{t=1}^T\\\\prod\\\\limits_{-m\\\\le j \\\\le m, j\\\\neq 0}P(\\\\color{#888}{w_{t+j}}|\\\\color{#88bd33}{w_t}\\\\color{black}, \\\\theta),'\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"where \", _jsx(Math, {\n          children: \"\\\\theta\"\n        }), \" are all variables to be optimized.\"]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/loss_with_the_plan-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Note how well the loss agrees with our plan main above: go over text with a\\nsliding window and compute probabilities.\\nNow let's find out how to compute these probabilities.\"\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"word2vec_calculate_p\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [_jsx(\"u\", {\n          children: \"How to calculate\"\n        }), \"  \", _jsx(Math, {\n          children: 'P(\\\\color{#888}{w_{t+j}}\\\\color{black}|\\\\color{#88bd33}{w_t}\\\\color{black}, \\\\theta)?'\n        })]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/two_vocs_with_theta-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(Quiz, {\n        name: 'test'\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["nlp-course","word-embeddings"]},"buildId":"EI9pVGJ5rW9mnqYgy4RyH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>