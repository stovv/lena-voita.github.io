{"pageProps":{"meta":{"title":"Test","layout":"course","contentTable":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      ul: \"ul\",\n      li: \"li\",\n      a: \"a\",\n      strong: \"strong\"\n    }, _provideComponents(), props.components), {Hl, Img} = _components;\n    if (!Hl) _missingMdxReference(\"Hl\", true);\n    if (!Img) _missingMdxReference(\"Img\", true);\n    return _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsxs(_components.a, {\n          href: \"#nlp-course-for-you\",\n          children: [_jsx(_components.strong, {\n            children: \"NLP Course\"\n          }), \" \", _jsx(Hl, {\n            color: \"green\",\n            children: _jsx(_components.strong, {\n              children: \"| For You\"\n            })\n          }), \" \", _jsx(Img, {\n            src: '/nlp-course/ico/logo.webp',\n            style: {\n              float: 'right',\n              width: '24px'\n            }\n          })]\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.strong, {\n          children: \"Word Embeddings\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#represent-as-discrete-symbols-one-hot-vectors\",\n          children: \"One-Hot Vectors\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#distributional-semantics\",\n          children: \"Distributional Semantics\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Count-Based Methods\"\n        })\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"#\",\n          children: \"Word2Vec\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Idea\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Objective Function\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Training Procedure\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Negative Sampling\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Skip-Gram vs CBOW\"\n            })\n          }), \"\\n\", _jsx(_components.li, {\n            children: _jsx(_components.a, {\n              href: \"#\",\n              children: \"Additional Notes\"\n            })\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"GloVe\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Evaluation\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Analysis and Interpretability\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Research Thinking\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Related Papers\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"#\",\n          children: \"Have Fun!\"\n        })\n      }), \"\\n\"]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"slug":["nlp-course","word-embeddings"],"mdxSource":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      strong: \"strong\",\n      h4: \"h4\",\n      h3: \"h3\",\n      a: \"a\",\n      blockquote: \"blockquote\",\n      h1: \"h1\",\n      img: \"img\",\n      ul: \"ul\",\n      li: \"li\",\n      h2: \"h2\"\n    }, _provideComponents(), props.components), {Img, Part, Callout, CourseCarousel, Cl, Math, BlockMath, Quiz} = _components;\n    if (!BlockMath) _missingMdxReference(\"BlockMath\", true);\n    if (!Callout) _missingMdxReference(\"Callout\", true);\n    if (!Cl) _missingMdxReference(\"Cl\", true);\n    if (!CourseCarousel) _missingMdxReference(\"CourseCarousel\", true);\n    if (!Img) _missingMdxReference(\"Img\", true);\n    if (!Math) _missingMdxReference(\"Math\", true);\n    if (!Part) _missingMdxReference(\"Part\", true);\n    if (!Quiz) _missingMdxReference(\"Quiz\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsx(\"h1\", {\n        children: \"Word Embeddings\"\n      }), \"\\n\", _jsx(Img, {\n        height: \"200\",\n        src: \"/nlp-course/word_emb/word_repr_intro-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The way machine learning models \\\"\", _jsx(_components.strong, {\n          children: \"see\"\n        }), \"\\\" data is different from how we (humans) do. For example, we can easily\\nunderstand the text \", _jsx(_components.strong, {\n          children: \"\\\"I saw a cat\\\"\"\n        }), \",\\nbut our models can not - they need vectors of features.\\nSuch vectors, or \", _jsx(_components.strong, {\n          children: \"word embeddings\"\n        }), \", are representations of words which can be fed into your model.\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(Img, {\n        height: \"130\",\n        src: \"/nlp-course/word_emb/lookup_table.gif\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(\"u\", {\n          children: \"How it works:\"\n        }), \" Look-up Table (Vocabulary)\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance.\\nFor each vocabulary word, a look-up table contains its embedding. This embedding can be found\\nusing the word index in the vocabulary (i.e., you\\nto \", _jsx(_components.strong, {\n          children: \"look up\"\n        }), \" the embedding in the table using word index).\"]\n      }), \"\\n\", _jsx(Img, {\n        height: \"70\",\n        src: \"/nlp-course/word_emb/unk_in_voc-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary\\ncontains a special token \", _jsx(_components.strong, {\n          children: \"UNK\"\n        }), \". Alternatively, unknown tokens can be ignored or assigned a zero vector.\"]\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"The main question of this lecture is: how do we get these word vectors?\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h3, {\n        children: \"Represent as Discrete Symbols: One-hot Vectors\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The easiest you can do is to represent words as one-hot vectors: for the i-th word in the vocabulary,\\nthe vector has 1 on the i-th dimension and 0 on the rest. In Machine Learning, this is the most simple way to represent\\ncategorical features.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"You probably can guess why one-hot vectors are not the best way to represent words. One of the problems is that\\nfor large vocabularies, these vectors will be very long: vector dimensionality is equal to the vocabulary size.\\nThis is undesirable in practice, but this problem is not the most crucial one.\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"What is really important, is that these vectors \", _jsx(_components.strong, {\n          children: \"know nothing\"\n        }), \"\\nabout the words they represent. For example, one-hot vectors \\\"think\\\" that \", _jsx(_components.strong, {\n          children: \"cat\"\n        }), \" is as close to \", _jsx(_components.strong, {\n          children: \"dog\"\n        }), \" as it is to \", _jsx(_components.strong, {\n          children: \"table!\"\n        }), \"\\nWe can say that \", _jsxs(\"u\", {\n          children: [\"one-hot vectors do not capture \", _jsx(_components.strong, {\n            children: \"meaning.\"\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"But how do we know what is meaning?\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h3, {\n        children: \"Distributional Semantics\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To capture meaning of words in their vectors, we first need to define\\nthe notion of meaning that can be used in practice.\\nFor this, let us try to understand how we, humans, get to know which words have similar meaning.\"\n      }), \"\\n\", _jsxs(Part, {\n        divider: {\n          topImage: '/nlp-course/ico/paw_empty.webp',\n          botImage: '/nlp-course/ico/paw_empty.webp'\n        },\n        children: [_jsx(Callout, {\n          color: 'var(--green)',\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"How to:\"\n            }), \" go over the slides at your pace. Try to notice how your brain works.\"]\n          })\n        }), _jsx(CourseCarousel, {\n          slides: [{\n            url: '/nlp-course/word_emb/tezguino1-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course//word_emb/tezguino2-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino3-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino4-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino5-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino6-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino7-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino8-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino9-min.png',\n            title: ''\n          }]\n        }), _jsx(Cl, {\n          gray: true,\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"Lena\"\n            }), \": The example is from \", _jsx(_components.a, {\n              href: \"https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf\",\n              children: \"Jacob Eisenstein's NLP notes;\"\n            }), \"\\nthe \", _jsx(_components.strong, {\n              children: \"tezgüino\"\n            }), \" example originally appeared in \", _jsx(_components.a, {\n              href: \"https://www.aclweb.org/anthology/C98-2122.pdf\",\n              children: \"Lin, 1998\"\n            }), \".\"]\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Once you saw how the unknown word used in different contexts, you were able to understand it's meaning.\\nHow did you do this?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The hypothesis is that your brain searched for other words\\nthat can be used in the same contexts, found some (e.g., \", _jsx(_components.strong, {\n          children: \"wine\"\n        }), \"), and\\nmade a conclusion that \", _jsx(_components.strong, {\n          children: \"tezgüino\"\n        }), \" has meaning similar to those other words.\\nThis is the \", _jsx(Cl, {\n          comic: true,\n          children: \"distributional hypothesis:\"\n        })]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Words which frequently appear in \", _jsx(_components.strong, {\n            children: \"similar contexts\"\n          }), \" have \", _jsx(_components.strong, {\n            children: \"similar meaning\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Once you saw how the unknown word used in different contexts,\\nyou were able to understand it's meaning.\\nHow did you do this?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The hypothesis is that your brain searched for other words\\nthat can be used in the same contexts, found some (e.g., \", _jsx(_components.strong, {\n          children: \"wine\"\n        }), \"), and\\nmade a conclusion that \", _jsx(_components.strong, {\n          children: \"tezgüino\"\n        }), \" has meaning\\nsimilar to those other words.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This is the \", _jsx(Cl, {\n          comic: true,\n          children: \"distributional hypothesis:\"\n        })]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [\"Words which frequently appear in \", _jsx(_components.strong, {\n            children: \"similar contexts\"\n          }), \" have \", _jsx(_components.strong, {\n            children: \"similar meaning\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(Cl, {\n        gray: true,\n        comic: true,\n        children: _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Lena:\"\n          }), \" Often you can find it formulated as \\\"You shall know a word by the company it keeps\\\" with the reference\\nto J. R. Firth in 1957, but\\nactually there\\nwere a lot more people\\nresponsible, and much earlier. For example, \", _jsx(_components.a, {\n            href: \"https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\",\n            children: \"Harris, 1954.\"\n          })]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This is an extremely valuable idea: it can be used in practice to make word vectors capture\\ntheir meaning. According to the distributional hypothesis, \\\"to capture meaning\\\" and\\n\\\"to capture contexts\\\" are inherently the same.\\nTherefore, all we need to do is to put information about word contexts into word representation.\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We need to put information about word contexts into word representation.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"All we'll be doing at this lecture is looking at different ways to do this.\"\n      }), \"\\n\", _jsx(_components.h1, {\n        children: \"Count-Based Methods\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"/nlp-course/word_emb/preneural/idea-min.png\",\n          alt: \"count-based\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let's remember our main idea:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We have to put information about contexts into word vectors.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Count-based methods take this idea quite literally:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How\"\n          }), \": Put this information \", _jsx(_components.strong, {\n            children: \"manually\"\n          }), \" based on global corpus statistics.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The general procedure is illustrated above and consists of the two steps: (1)\\nconstruct a word-context matrix, (2) reduce its dimensionality. There are two reasons to reduce dimensionality.\\nFirst, a raw matrix is very large. Second, since a lot of words appear in only a few of possible contexts,\\nthis matrix potentially has a lot of uninformative elements (e.g., zeros).\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To estimate\\nsimilarity between words/contexts, usually you need to evaluate\\nthe dot-product of normalized word/context vectors (i.e., cosine similarity).\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/need_to_define-min.png\",\n        right: true,\n        lightBox: true,\n        style: {\n          width: '25%'\n        }\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To define a count-based method, we need to define two things:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"possible contexts (including what does it mean that a word appears in a context),\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"the notion of association, i.e., formulas for computing matrix elements.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Below we provide a couple of popular ways of doing this.\"\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Simple: Co-Occurence Counts\"\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/window-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/define_simple-min.png\",\n        right: true,\n        lightBox: true,\n        style: {\n          width: '25%'\n        }\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The simplest approach is to define contexts as each word in an L-sized window.\\nMatrix element for a word-context pair (w, c) is the number of times w appears in context c.\\nThis is the very basic (and very, very old) method for obtaining embeddings.\"\n      }), \"\\n\", _jsx(Callout, {\n        icon: 'bulb_empty',\n        children: _jsxs(_components.p, {\n          children: [\"The (once) famous HAL model (1996)\\nis also a modification of this approach.\\nLearn more from \", _jsx(_components.a, {\n            href: \"#research_improve_count_based\",\n            children: \"this exercise\"\n          }), \"\\nin the \", _jsx(_components.a, {\n            href: \"#research_thinking\",\n            children: \"Research Thinking\"\n          }), \" section.\"]\n        })\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Positive Pointwise Mutual Information (PPMI)\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/define_ppmi-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Here contexts are defined as before, but the measure of\\nthe association between word and context is more clever: positive PMI (or PPMI for short).\\nPPMI measure is widely regarded as state-of-the-art for pre-neural distributional-similarity models.\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Important\"\n          }), \": relation to neural models! Turns out, some of the neural methods we will consider (Word2Vec) were shown\\nto implicitly approximate the factorization of a (shifted) PMI matrix. Stay tuned!\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(_components.h2, {\n        children: \"Latent Semantic Analysis (LSA): Understanding Documents\"\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/preneural/lsa-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(_components.a, {\n          href: \"http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf\",\n          children: \"Latent Semantic Analysis (LSA)\"\n        }), \" analyzes a collection of documents.\\nWhile in the previous approaches contexts served only to get word vectors and were thrown away afterward, here we are\\nalso interested in context, or, in this case, document vectors. LSA is one of the simplest topic models:\\ncosine similarity between document vectors can be used to measure similarity between documents.\\nThe term \\\"LSA\\\" sometimes refers to a more general approach of applying SVD to a term-document\\nmatrix where the term-document elements can be computed in different ways\\n(e.g., simple co-occurrence, tf-idf, or some other weighting).\"]\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Animation alert!\"\n          }), \"\\n\", _jsx(_components.a, {\n            href: \"https://en.wikipedia.org/wiki/Latent_semantic_analysis\",\n            children: \"LSA wikipedia page\"\n          }), \" has a nice\\nanimation of the topic detection process in a document-word matrix - take a look!\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.h1, {\n        children: \"Word2Vec: a Prediction-Based Method\"\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"w2v_idea\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let us remember our main idea again:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"Main idea\"\n          }), \": We have to put information about contexts into word vectors.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"While count-based methods took this idea quite literally, Word2Vec uses it in a different manner:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How\"\n          }), \": \", _jsx(_components.strong, {\n            children: \"Learn\"\n          }), \" word vectors by teaching them to \", _jsx(_components.strong, {\n            children: \"predict contexts\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/intro-min.png\",\n        right: true,\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively\\nfor a certain objective. The objective forces word vectors to \\\"know\\\" contexts a word can appear in:\\nthe vectors are trained to predict possible contexts of the corresponding words.\\nAs you remember from the distributional hypothesis, if vectors \\\"know\\\" about contexts, they \\\"know\\\" word meaning.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Word2Vec is an iterative method. Its main idea is as follows:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"take a huge text corpus;\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"go over the text with a sliding window, moving one word at a time. At each step, there is a central word\\nand context words (other words in this window);\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"for the central word, compute probabilities of context words;\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"adjust the vectors to increase these probabilities.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(Part, {\n        divider: {\n          topImage: '/nlp-course/ico/paw_empty.webp',\n          botImage: '/nlp-course/ico/paw_empty.webp'\n        },\n        children: [_jsxs(_components.p, {\n          children: [_jsx(\"u\", {\n            children: \"How to:\"\n          }), \" go over the illustration to understand the main idea.\"]\n        }), _jsx(CourseCarousel, {\n          slides: [{\n            url: '/nlp-course/word_emb/w2v/window_prob1-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob2-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob3-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob4-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob5-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/w2v/window_prob6-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino7-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino8-min.png',\n            title: ''\n          }, {\n            url: '/nlp-course/word_emb/tezguino9-min.png',\n            title: ''\n          }]\n        }), _jsx(Cl, {\n          gray: true,\n          children: _jsxs(_components.p, {\n            children: [_jsx(\"u\", {\n              children: \"Lena:\"\n            }), \" Visualization idea is from \", _jsx(_components.a, {\n              href: \"http://web.stanford.edu/class/cs224n/index.html#schedule\",\n              children: \"the Stanford CS224n course.\"\n            })]\n          })\n        })]\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"w2v_objective_function\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [_jsx(\"u\", {\n          children: \"Objective Function\"\n        }), \": Negative Log-Likelihood\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"For each position \", _jsx(Math, {\n          children: \"(t =1, \\\\dots, T)\"\n        }), \" in a text corpus,\\nWord2Vec predicts context words within a m-sized window given the central\\nword \", _jsx(Math, {\n          children: '\\\\color{#88bd33}{w_t}'\n        }), \":\"]\n      }), \"\\n\", _jsx(\"br\", {}), \"\\n\", _jsx(BlockMath, {\n        children: '\\\\color{#88bd33}{{Likelihood}} \\\\color{black}= L(\\\\theta)=\\\\prod\\\\limits_{t=1}^T\\\\prod\\\\limits_{-m\\\\le j \\\\le m, j\\\\neq 0}P(\\\\color{#888}{w_{t+j}}|\\\\color{#88bd33}{w_t}\\\\color{black}, \\\\theta),'\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"where \", _jsx(Math, {\n          children: \"\\\\theta\"\n        }), \" are all variables to be optimized.\"]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/loss_with_the_plan-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Note how well the loss agrees with our plan main above: go over text with a\\nsliding window and compute probabilities.\\nNow let's find out how to compute these probabilities.\"\n      }), \"\\n\", _jsx(\"div\", {\n        id: \"word2vec_calculate_p\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [_jsx(\"u\", {\n          children: \"How to calculate\"\n        }), \"  \", _jsx(Math, {\n          children: 'P(\\\\color{#888}{w_{t+j}}\\\\color{black}|\\\\color{#88bd33}{w_t}\\\\color{black}, \\\\theta)?'\n        })]\n      }), \"\\n\", _jsx(Img, {\n        src: \"/nlp-course/word_emb/w2v/two_vocs_with_theta-min.png\",\n        lightBox: true\n      }), \"\\n\", _jsx(Quiz, {\n        name: 'test'\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}}},"__N_SSG":true}