---
layout: post
title: Neural Machine Translation Inside Out
description: Lena Voita's keynote at ACL 2021 workshop RepL4NLP and NAACL 2021 workshop DeeLIO.
image: /posts/nmt-inside-out/morda_test.webp
menu: true
order: 1
date: 2021-07-01
---

<Preview>
  <SideContainer>
    <Main md>
      <Cl>
        This is a blog version of my talk at the ACL 2021 workshop
        [Representation Learning for NLP](https://sites.google.com/view/repl4nlp-2021/) (and updated version
        of that at NAACL 2021 workshop [Deep Learning Inside Out (DeeLIO)](https://sites.google.com/view/deelio-ws/) ).
      </Cl>
      In the last decade, machine translation shifted from the traditional statistical approaches
      with distinct components and hand-crafted features to the end-to-end neural ones.
      We try to understand how NMT works and show that:
      - NMT model components can learn to extract features which in SMT were modelled explicitly;
      - for NMT, we can also look at how it balances the two different types of context: the source and the prefix;
      - NMT training consists of the stages where it focuses on competences mirroring three core SMT components.
    </Main>
    <Side>
      ![](/posts/nmt-inside-out/morda_test.webp)
    </Side>
  </SideContainer>
</Preview>

---
# Test
